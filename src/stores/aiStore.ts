import { create } from 'zustand';
import { api } from '../api/api';
import { AIConversationRequestDto, ConversationMessage, FileOperationDto, OpenFileContext, AIPreferences } from '../api/types';

export interface Message {
  id: string;
  role: 'user' | 'assistant';
  content: string;
  timestamp: Date;
  fileOperations?: FileOperationDto[];
  warnings?: string[];
  suggestedFollowUps?: string[];
}

export interface AIPreferencesState {
  verbosity: 'concise' | 'normal' | 'detailed';
  explainReasoning: boolean;
  suggestBestPractices: boolean;
  maxFileOperations: number;
  contextMode: 'aggressive' | 'balanced' | 'comprehensive' | 'unlimited';
}

interface AIStore {
  // State
  conversationHistory: Message[];
  isThinking: boolean;
  error: string | null;
  currentProgramId: string | null;
  currentVersionId: string | null;
  preferences: AIPreferencesState;
  suggestedFollowUps: string[];

  // Actions
  sendMessage: (userPrompt: string, openFileContexts?: OpenFileContext[]) => Promise<FileOperationDto[]>;
  clearConversation: () => void;
  setCurrentProgram: (programId: string, versionId?: string) => void;
  setError: (error: string | null) => void;
  updatePreferences: (preferences: Partial<AIPreferencesState>) => void;
}

export const useAIStore = create<AIStore>((set, get) => ({
  // Initial state
  conversationHistory: [],
  isThinking: false,
  error: null,
  currentProgramId: null,
  currentVersionId: null,
  preferences: {
    verbosity: 'normal',
    explainReasoning: true,
    suggestBestPractices: true,
    maxFileOperations: 5,
    contextMode: 'balanced',
  },
  suggestedFollowUps: [],

  // Actions
  sendMessage: async (userPrompt: string, openFileContexts?: OpenFileContext[]) => {
    const state = get();

    if (!state.currentProgramId) {
      const error = 'No program selected. Please select a program first.';
      set({ error });
      throw new Error(error);
    }

    // Add user message to history
    const userMessage: Message = {
      id: crypto.randomUUID(),
      role: 'user',
      content: userPrompt,
      timestamp: new Date(),
    };

    set({
      conversationHistory: [...state.conversationHistory, userMessage],
      isThinking: true,
      error: null
    });

    try {
      // Prepare conversation history for API (ConversationMessage format)
      const conversationMessages: ConversationMessage[] = state.conversationHistory.map(msg => {
        const convMsg = new ConversationMessage();
        convMsg.role = msg.role;
        convMsg.content = msg.content;
        convMsg.timestamp = msg.timestamp;
        convMsg.fileOperations = msg.fileOperations;
        return convMsg;
      });

      // Add the new user message to conversation history
      const newUserMsg = new ConversationMessage();
      newUserMsg.role = 'user';
      newUserMsg.content = userPrompt;
      newUserMsg.timestamp = new Date();
      conversationMessages.push(newUserMsg);

      // Build AI preferences
      const aiPreferences = new AIPreferences({
        verbosity: state.preferences.verbosity,
        explainReasoning: state.preferences.explainReasoning,
        suggestBestPractices: state.preferences.suggestBestPractices,
        maxFileOperations: state.preferences.maxFileOperations,
        contextMode: state.preferences.contextMode,
      });

      // Create request
      const request = new AIConversationRequestDto({
        userPrompt,
        programId: state.currentProgramId,
        versionId: state.currentVersionId || undefined,
        conversationHistory: conversationMessages,
        currentlyOpenFiles: openFileContexts,
        preferences: aiPreferences,
      });

      // Call API
      const response = await api.aiAssistantClient.aIAssistant_Converse(request);

      if (!response.success || !response.data) {
        throw new Error(response.message || 'Failed to get response from AI');
      }

      const aiResponse = response.data;

      // Convert file operations from interface to class instances
      const fileOperations = aiResponse.fileOperations || [];

      // Add assistant message to history
      const assistantMessage: Message = {
        id: crypto.randomUUID(),
        role: 'assistant',
        content: aiResponse.displayText || '',
        timestamp: new Date(),
        fileOperations: fileOperations as FileOperationDto[],
        warnings: aiResponse.warnings,
        suggestedFollowUps: aiResponse.suggestedFollowUps,
      };

      set({
        conversationHistory: [...get().conversationHistory, assistantMessage],
        isThinking: false,
        suggestedFollowUps: aiResponse.suggestedFollowUps || [],
      });

      // Return file operations for editor integration
      return fileOperations as FileOperationDto[];
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'An unknown error occurred';
      set({
        error: errorMessage,
        isThinking: false
      });
      throw error;
    }
  },

  clearConversation: () => {
    set({
      conversationHistory: [],
      error: null
    });
  },

  setCurrentProgram: (programId: string, versionId?: string) => {
    set({
      currentProgramId: programId,
      currentVersionId: versionId,
      // Clear conversation when switching programs
      conversationHistory: [],
      error: null
    });
  },

  setError: (error: string | null) => {
    set({ error });
  },

  updatePreferences: (preferences: Partial<AIPreferencesState>) => {
    set(state => ({
      preferences: {
        ...state.preferences,
        ...preferences,
      },
    }));
  },
}));


// ðŸ“‹ Next Steps for You

//   1. Set Up Environment Variables

//   Create or update .env file in TeiasMongoAPI.API directory:

//   # Gemini API Key (required for both LLM and embeddings)
//   GEMINI_API_KEY=your-gemini-api-key-here

//   2. Start Qdrant (if using vector search)

//   # From the TeiasMongoAPI root directory
//   docker-compose -f docker-compose.qdrant.yml up -d

//   # Verify Qdrant is running
//   curl http://localhost:6333/health

//   3. Update appsettings.json

//   The LLM and VectorStore sections are already configured! Just update:
//   - LLM:ApiKey - Your Gemini API key (or use environment variable)
//   - VectorStore:EnableVectorSearch - Set to false if you don't want vector search initially

//   4. Build the Project

//   dotnet restore
//   dotnet build

//   5. Test the Implementation

//   The AI assistant will now:
//   - âœ… See ALL files in the project via the code index
//   - âœ… Handle unsaved editor changes
//   - âœ… Use 40-90K tokens of context (configurable)
//   - âœ… Optionally use semantic search to find relevant code

//   6. Index Your First Project (for vector search)

//   You'll need to create an indexing endpoint or background job to:
//   1. Call ICodeChunker.ChunkProjectAsync(programId, versionId)
//   2. Call IEmbeddingService.GenerateEmbeddingsAsync() for each chunk
//   3. Call IVectorStore.UpsertChunksAsync() to store them

//   ---